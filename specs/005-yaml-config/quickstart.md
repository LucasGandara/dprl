# Quickstart: YAML Configuration File Support

**Feature**: 005-yaml-config
**Date**: 2026-01-27

## Overview

This feature adds type-safe YAML configuration file support to DPRL algorithms
using a SOLID-compliant architecture. Each algorithm (e.g., VPG) owns its
config model, inheriting from a reusable base class.

## Usage

### Generate a Configuration Template

```bash
uv run python examples/vpg_cartpole.py --generate-config
```

This creates `vpg_config_template.yaml` with all options and defaults:

```yaml
# VPG Training Configuration
# Generated by: python vpg_cartpole.py --generate-config

# Number of epochs to train for
epochs: 50

# Learning rate
lr: 0.001

# Number of units in the hidden layer
hidden-layer-units: 64

# Advantage expression to use
# Valid values: total_reward, reward_to_go, baselined
advantage-expression: reward_to_go
```

### Run with a Configuration File

```bash
uv run python examples/vpg_cartpole.py --config my_config.yaml
```

### Override Config Values with CLI Arguments

CLI arguments take precedence over config file values:

```bash
# Uses config file but overrides epochs to 100
uv run python examples/vpg_cartpole.py --config my_config.yaml --epochs 100
```

### Backward Compatibility

Existing CLI usage continues to work unchanged:

```bash
uv run python examples/vpg_cartpole.py --epochs 800 --lr 0.001
```

## Type Safety

Configuration is validated using Pydantic models:

- **Type coercion**: `epochs: "50"` automatically converts to `50`
- **Constraint validation**: `epochs: 0` fails with "must be ≥1"
- **Enum validation**: `advantage-expression: invalid` fails with valid options
- **Unknown field detection**: `epocs: 50` fails listing valid fields

## For Algorithm Developers

### Creating a Config for a New Algorithm

1. Create a config file in your algorithm module:

```python
# src/dprl/algorithms/my_algo/config.py
from pydantic import Field
from typing import Literal
from dprl.utils.config import BaseConfig

class MyAlgoConfig(BaseConfig):
    """Configuration for My Algorithm."""

    learning_rate: float = Field(
        default=0.001,
        gt=0,
        alias="learning-rate",
        description="Learning rate for optimization"
    )

    batch_size: int = Field(
        default=32,
        ge=1,
        alias="batch-size",
        description="Training batch size"
    )

    optimizer: Literal["adam", "sgd", "rmsprop"] = Field(
        default="adam",
        description="Optimizer to use"
    )
```

2. Export from your algorithm's `__init__.py`:

```python
# src/dprl/algorithms/my_algo/__init__.py
from .config import MyAlgoConfig

__all__ = ["MyAlgoConfig", ...]
```

3. Use in your example script:

```python
# examples/my_algo_example.py
from dprl.algorithms.my_algo import MyAlgoConfig
from dprl.utils.config import config_option, generate_config_option

@click.command()
@config_option(MyAlgoConfig)
@generate_config_option(MyAlgoConfig)
@click.option("--learning-rate", default=0.001)
@click.option("--batch-size", default=32)
@click.option("--optimizer", default="adam")
def train(learning_rate, batch_size, optimizer):
    # Values come from: CLI args > config file > defaults
    ...
```

### SOLID Principles Applied

- **Single Responsibility**: BaseConfig handles I/O; your config defines fields
- **Open/Closed**: Extend BaseConfig, don't modify it
- **Liskov Substitution**: Your config works with all base utilities
- **Interface Segregation**: Only implement what you need (just fields)
- **Dependency Inversion**: Examples depend on BaseConfig abstraction

## API Reference

### BaseConfig Methods

```python
# Load from YAML file
config = VPGConfig.load_from_yaml(Path("config.yaml"))

# Generate template with comments
VPGConfig.generate_template(Path("template.yaml"))

# Convert to Click default_map
default_map = config.to_click_default_map()
```

### Click Decorators

```python
from dprl.utils.config import config_option, generate_config_option

# Add --config option
@config_option(VPGConfig)

# Add --generate-config option
@generate_config_option(VPGConfig)
```

## Error Handling

### Unknown Option

```
Configuration Error in 'config.yaml':

  epocs: Extra inputs are not permitted.
         Valid fields: epochs, lr, hidden-layer-units, advantage-expression
```

### Invalid Value

```
Configuration Error in 'config.yaml':

  advantage-expression: Input should be 'total_reward', 'reward_to_go' or
                        'baselined'
                        Got: 'invalid_method'
```

### Invalid Type

```
Configuration Error in 'config.yaml':

  epochs: Input should be a valid integer
          Got: 'not a number'
```

---

## Phase 2: Config Saving with Experiments

### Automatic Config Saving

When running an experiment, the configuration is automatically saved alongside
the model checkpoint for full reproducibility:

```bash
uv run python examples/vpg_cartpole.py --config my_config.yaml
```

Creates:

```
runs/exp_vpg_20260127_143022/
├── policy.tar        # Model weights
└── config.yaml       # Training configuration
```

### Loading a Saved Configuration

To reproduce an experiment, load both the model and its configuration:

```python
from dprl.utils import load_experiment_details, load_config_from_experiment
from dprl.algorithms.vpg import VPGConfig

# Load the model
checkpoint = load_experiment_details("runs/exp_vpg_20260127_143022/policy.tar")

# Load the configuration
config = load_config_from_experiment(
    "runs/exp_vpg_20260127_143022",
    VPGConfig
)

if config:
    print(f"Original training used {config.epochs} epochs")
    print(f"Learning rate was {config.lr}")
```

### Reproducibility Workflow

1. **Run experiment** with config:
   ```bash
   uv run python examples/vpg_cartpole.py --config experiment.yaml
   ```

2. **Config is saved** automatically in `runs/exp_vpg_*/config.yaml`

3. **Reproduce later** by copying the saved config:
   ```bash
   cp runs/exp_vpg_20260127_143022/config.yaml reproduce.yaml
   uv run python examples/vpg_cartpole.py --config reproduce.yaml
   ```

### API for Config Saving

```python
from dprl.utils import save_experiment_details
from dprl.algorithms.vpg import VPGConfig

# Create config from effective values
config = VPGConfig(
    epochs=epochs,
    lr=lr,
    hidden_layer_units=hidden_layer_units,
    advantage_expression=advantage_expression,
)

# Save experiment with config
save_experiment_details(
    name="vpg",
    policy=value_function,
    config=config,  # NEW: saves config.yaml alongside policy.tar
    aditional_data={...},
)
```
