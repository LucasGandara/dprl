# Data Model: YAML Configuration File Support

**Feature**: 005-yaml-config
**Date**: 2026-01-27

## Class Hierarchy

```
BaseConfig (src/dprl/utils/config.py)
│   - Abstract base for all algorithm configs
│   - Provides: load_from_yaml(), generate_template(), to_click_default_map()
│
└── VPGConfig (src/dprl/algorithms/vpg/config.py)
        - Concrete config for VPG algorithm
        - Fields: epochs, lr, hidden_layer_units, advantage_expression
```

## BaseConfig (Abstract Base)

```python
from pydantic import BaseModel, ConfigDict
from pathlib import Path
from typing import Self, Any

class BaseConfig(BaseModel):
    """
    Base configuration class for all DPRL algorithms.

    Subclasses define algorithm-specific fields. This base class provides:
    - YAML loading with Pydantic validation
    - Template generation with comments
    - Click default_map conversion for CLI integration
    """

    model_config = ConfigDict(
        populate_by_name=True,  # Accept both alias and field name
        extra="forbid",         # Reject unknown fields
    )

    @classmethod
    def load_from_yaml(cls, path: Path) -> Self:
        """Load and validate configuration from YAML file."""
        ...

    @classmethod
    def generate_template(cls, path: Path) -> None:
        """Generate YAML template with defaults and comments."""
        ...

    def to_click_default_map(self) -> dict[str, Any]:
        """Convert to Click default_map format (using aliases)."""
        ...
```

**Methods**:

| Method | Type | Description |
|--------|------|-------------|
| `load_from_yaml(path)` | classmethod | Parse YAML and validate with Pydantic |
| `generate_template(path)` | classmethod | Write commented YAML with defaults |
| `to_click_default_map()` | instance | Convert to dict for Click integration |

## VPGConfig (Concrete Implementation)

```python
from pydantic import Field
from typing import Literal
from dprl.utils.config import BaseConfig

class VPGConfig(BaseConfig):
    """Configuration for Vanilla Policy Gradient (VPG) algorithm."""

    epochs: int = Field(
        default=50,
        ge=1,
        description="Number of epochs to train for"
    )

    lr: float = Field(
        default=0.001,
        gt=0,
        description="Learning rate"
    )

    hidden_layer_units: int = Field(
        default=64,
        ge=1,
        alias="hidden-layer-units",
        description="Number of units in the hidden layer"
    )

    advantage_expression: Literal[
        "total_reward",
        "reward_to_go",
        "baselined"
    ] = Field(
        default="reward_to_go",
        alias="advantage-expression",
        description="Advantage expression to use"
    )
```

**Fields**:

| Field | Type | Default | Constraints | YAML Key |
|-------|------|---------|-------------|----------|
| epochs | int | 50 | ≥1 | epochs |
| lr | float | 0.001 | >0 | lr |
| hidden_layer_units | int | 64 | ≥1 | hidden-layer-units |
| advantage_expression | Literal | reward_to_go | enum | advantage-expression |

## Validation Behavior

**Type Coercion** (automatic):
- `epochs: "50"` → `epochs: 50`
- `lr: "0.001"` → `lr: 0.001`

**Validation Errors**:
- Unknown field → `extra_forbidden` with valid fields list
- Wrong type → `int_parsing` / `float_parsing` with expected type
- Invalid choice → `literal_error` with allowed values
- Constraint violation → `greater_than` / `greater_than_equal`

## Configuration File Format

### Valid YAML Example

```yaml
# VPG Training Configuration
epochs: 100
lr: 0.0005
hidden-layer-units: 128
advantage-expression: baselined
```

### Generated Template

```yaml
# VPG Training Configuration
# Generated by: python vpg_cartpole.py --generate-config

# Number of epochs to train for
epochs: 50

# Learning rate
lr: 0.001

# Number of units in the hidden layer
hidden-layer-units: 64

# Advantage expression to use
# Valid values: total_reward, reward_to_go, baselined
advantage-expression: reward_to_go
```

## Click Integration

### Decorator Factory

```python
def config_option(config_class: type[BaseConfig]):
    """
    Decorator factory for --config CLI option.

    Usage:
        @config_option(VPGConfig)
        @click.command()
        def my_command(...):
            ...
    """
    ...

def generate_config_option(config_class: type[BaseConfig]):
    """
    Decorator factory for --generate-config CLI option.

    Usage:
        @generate_config_option(VPGConfig)
        @click.command()
        def my_command(...):
            ...
    """
    ...
```

### Usage in Examples

```python
from dprl.algorithms.vpg import VPGConfig
from dprl.utils.config import config_option, generate_config_option

@click.command()
@config_option(VPGConfig)
@generate_config_option(VPGConfig)
@click.option("--epochs", default=50, help="Number of epochs")
def vpg_cartpole(epochs, lr, hidden_layer_units, advantage_expression):
    # Values come from: CLI args > config file > defaults
    ...
```

## Error Messages

### Unknown Field

```
Configuration Error in 'config.yaml':

  epocs: Extra inputs are not permitted.
         Valid fields: epochs, lr, hidden-layer-units, advantage-expression
```

### Invalid Type

```
Configuration Error in 'config.yaml':

  epochs: Input should be a valid integer
          Got: 'not a number'
```

### Invalid Choice

```
Configuration Error in 'config.yaml':

  advantage-expression: Input should be 'total_reward', 'reward_to_go' or
                        'baselined'
                        Got: 'invalid_method'
```

---

## Phase 2: Config Saving with Experiments

### New Method: BaseConfig.to_yaml()

```python
def to_yaml(self, path: Path) -> None:
    """
    Save configuration to YAML file.

    Args:
        path: Path where YAML will be written.
    """
    data = self.model_dump(by_alias=True)
    with open(path, 'w') as f:
        yaml.safe_dump(data, f, default_flow_style=False, sort_keys=False)
```

### Modified: save_experiment_details()

```python
def save_experiment_details(
    policy: torch.nn.Module,
    aditional_data: dict[str, Optional[np.ndarray]] = {},
    name: str = "",
    config: Optional[BaseConfig] = None,  # NEW PARAMETER
) -> None:
    """
    Save experiment checkpoint with optional configuration.

    Args:
        policy: The trained policy network.
        aditional_data: Optional metrics (rewards, losses, etc.).
        name: Experiment name prefix.
        config: Optional config to save for reproducibility.
    """
    # ... existing logic ...

    # NEW: Save config if provided
    if config is not None:
        config_path = os.path.join(folder_name, "config.yaml")
        config.to_yaml(Path(config_path))
```

### New Function: load_config_from_experiment()

```python
def load_config_from_experiment(
    experiment_path: str,
    config_class: type[BaseConfig],
) -> BaseConfig | None:
    """
    Load configuration from a saved experiment.

    Args:
        experiment_path: Path to experiment folder or policy.tar file.
        config_class: The config class to use for validation.

    Returns:
        Loaded config if config.yaml exists, None otherwise.
    """
    folder = Path(experiment_path)
    if folder.is_file():
        folder = folder.parent

    config_path = folder / "config.yaml"
    if config_path.exists():
        return config_class.load_from_yaml(config_path)
    return None
```

### Updated Experiment Folder Structure

```
runs/
└── exp_vpg_20260127_143022/
    ├── policy.tar           # Model weights (existing)
    └── config.yaml          # NEW: Training configuration
```

### Saved config.yaml Format

```yaml
epochs: 100
lr: 0.0005
hidden-layer-units: 128
advantage-expression: baselined
```
